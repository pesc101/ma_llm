################################################################################
# Model & Dataset
################################################################################

model:
  base_model: "mistralai/Mistral-7B-Instruct-v0.2"
  new_model: "Mistral-7B-Instruct-v0.2-lbl-2x"
  device_map: "auto"
  use_cache: false
  tokenizer_padding: "left"


dataset:
  name: "spyder-ide-lbl-all-2x"
  split: "train"

################################################################################
# LoRA parameters
################################################################################

lora:
  # LoRA attention dimension
  lora_r: 64
  # Alpha parameter for LoRA scaling
  lora_alpha: 64
  # Dropout probability for LoRA layers
  lora_dropout: 0.1

################################################################################
# bitsandbytes parameters
################################################################################

bitsandbytes:
  # Activate 4-bit precision base model loading
  use_4bit: true
  # Compute dtype for 4-bit base models
  bnb_4bit_compute_dtype: "float16"
  # Quantization type (fp4 or nf4)
  bnb_4bit_quant_type: "nf4"
  # Activate nested quantization for 4-bit base models (double quantization)
  use_nested_quant: false

################################################################################
# TrainingArguments parameters
################################################################################

training_args:
  # Number of training epochs
  num_train_epochs: 5
  # Number of training steps (overrides num_train_epochs)
  max_steps: -1
  # Output directory where the model predictions and checkpoints will be stored
  output_dir: "./train/results"
  # Enable fp16/bf16 training (set bf16 to True with an A100)
  fp16: false
  bf16: true
  # Batch size per GPU for training
  per_device_train_batch_size: 32
  # Batch size per GPU for evaluation
  per_device_eval_batch_size: 4
  # Number of update steps to accumulate the gradients for
  gradient_accumulation_steps: 1
  # Enable gradient checkpointing
  gradient_checkpointing: true
  # Maximum gradient normal (gradient clipping)
  max_grad_norm: 0.3
  # Initial learning rate (AdamW optimizer)
  learning_rate: 2e-4
  # Weight decay to apply to all layers except bias/LayerNorm weights
  weight_decay: 0.001
  # Optimizer to use
  optim: "paged_adamw_32bit"
  # Learning rate schedule
  lr_scheduler_type: "cosine"
  # Ratio of steps for a linear warmup (from 0 to learning rate)
  warmup_ratio: 0.03
  # Group sequences into batches with same length
  # Saves memory and speeds up training considerably
  group_by_length: true
  # Save checkpoint every X updates steps
  save_steps: 0
  # Log every X updates steps
  logging_steps: 25

################################################################################
# SFTT parameters
################################################################################

sftt:
  # Maximum sequence length to use
  max_seq_length: 1024
  # Pack multiple short examples in the same input sequence to increase efficiency
  packing: false
